{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nid_op = ['none','nor_conv_1x1','nor_conv_3x3','skip_connect','avg_pool_3x3']\n\nnode_ops = [[]]*10\nnode_ops[0] = ['none','nor_conv_1x1','nor_conv_3x3','skip_connect','avg_pool_3x3']\nnode_ops[1] = ['none']\nnode_ops[2] = ['nor_conv_1x1','nor_conv_3x3','skip_connect','avg_pool_3x3']\nnode_ops[3] = ['nor_conv_1x1','nor_conv_3x3']\nnode_ops[4] = ['skip_connect','avg_pool_3x3']\nnode_ops[5] = ['nor_conv_1x1']\nnode_ops[6] = ['nor_conv_3x3']\nnode_ops[7] = ['skip_connect']\nnode_ops[8] = ['avg_pool_3x3']\n\nL = [(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)]\n\n'''\nReferences: NATS-Bench source code (on topology search space (tss)): \nhttps://github.com/D-X-Y/AutoDL-Projects/tree/f46486e21b71ae6459a700be720d7648b5429569/xautodl\n\nThe following classes are modified compared to original version: MixedOp, Cell\n\nArch (seems to) play the role of Gentotypes in the original implementation\nCell (seems to) play the role of InferCell in the original implementation\n'''\n\nOPS = {\n    \"none\": lambda C_in, C_out, stride, affine, track_running_stats: Zero(\n        C_in, C_out, stride\n    ),\n    \"avg_pool_3x3\": lambda C_in, C_out, stride, affine, track_running_stats: POOLING(\n        C_in, C_out, stride, \"avg\", affine, track_running_stats\n    ),\n    \"nor_conv_3x3\": lambda C_in, C_out, stride, affine, track_running_stats: ReLUConvBN(\n        C_in,\n        C_out,\n        (3, 3),\n        (stride, stride),\n        (1, 1),\n        (1, 1),\n        affine,\n        track_running_stats,\n    ),\n    \"nor_conv_1x1\": lambda C_in, C_out, stride, affine, track_running_stats: ReLUConvBN(\n        C_in,\n        C_out,\n        (1, 1),\n        (stride, stride),\n        (0, 0),\n        (1, 1),\n        affine,\n        track_running_stats,\n    ),\n    \"skip_connect\": lambda C_in, C_out, stride, affine, track_running_stats: Identity()\n    if stride == 1 and C_in == C_out\n    else FactorizedReduce(C_in, C_out, stride, affine, track_running_stats),\n}\n\nclass ReLUConvBN(nn.Module):\n    def __init__(\n        self,\n        C_in,\n        C_out,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        affine,\n        track_running_stats=True,\n    ):\n        super(ReLUConvBN, self).__init__()\n        self.op = nn.Sequential(\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                C_in,\n                C_out,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=dilation,\n                bias=not affine,\n            ),\n            nn.BatchNorm2d(\n                C_out, affine=affine, track_running_stats=track_running_stats\n            ),\n        )\n\n    def forward(self, x):\n        return self.op(x)\n\nclass POOLING(nn.Module):\n    def __init__(\n        self, C_in, C_out, stride, mode, affine=True, track_running_stats=True\n    ):\n        super(POOLING, self).__init__()\n        if C_in == C_out:\n            self.preprocess = None\n        else:\n            self.preprocess = ReLUConvBN(\n                C_in, C_out, 1, 1, 0, 1, affine, track_running_stats\n            )\n        if mode == \"avg\":\n            self.op = nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False)\n        elif mode == \"max\":\n            self.op = nn.MaxPool2d(3, stride=stride, padding=1)\n        else:\n            raise ValueError(\"Invalid mode={:} in POOLING\".format(mode))\n\n    def forward(self, inputs):\n        if self.preprocess:\n            x = self.preprocess(inputs)\n        else:\n            x = inputs\n        return self.op(x)\n\nclass Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, x):\n        return x\n\nclass Zero(nn.Module):\n    def __init__(self, C_in, C_out, stride):\n        super(Zero, self).__init__()\n        self.C_in = C_in\n        self.C_out = C_out\n        self.stride = stride\n        self.is_zero = True\n\n    def forward(self, x):\n        if self.C_in == self.C_out:\n            if self.stride == 1:\n                return x.mul(0.0)\n            else:\n                return x[:, :, :: self.stride, :: self.stride].mul(0.0)\n        else:\n            shape = list(x.shape)\n            shape[1] = self.C_out\n            zeros = x.new_zeros(shape, dtype=x.dtype, device=x.device)\n            return zeros\n\n    def extra_repr(self):\n        return \"C_in={C_in}, C_out={C_out}, stride={stride}\".format(**self.__dict__)\n\nclass FactorizedReduce(nn.Module):\n    def __init__(self, C_in, C_out, stride, affine, track_running_stats):\n        super(FactorizedReduce, self).__init__()\n        self.stride = stride\n        self.C_in = C_in\n        self.C_out = C_out\n        self.relu = nn.ReLU(inplace=False)\n        if stride == 2:\n            # assert C_out % 2 == 0, 'C_out : {:}'.format(C_out)\n            C_outs = [C_out // 2, C_out - C_out // 2]\n            self.convs = nn.ModuleList()\n            for i in range(2):\n                self.convs.append(\n                    nn.Conv2d(\n                        C_in, C_outs[i], 1, stride=stride, padding=0, bias=not affine\n                    )\n                )\n            self.pad = nn.ConstantPad2d((0, 1, 0, 1), 0)\n        elif stride == 1:\n            self.conv = nn.Conv2d(\n                C_in, C_out, 1, stride=stride, padding=0, bias=not affine\n            )\n        else:\n            raise ValueError(\"Invalid stride : {:}\".format(stride))\n        self.bn = nn.BatchNorm2d(\n            C_out, affine=affine, track_running_stats=track_running_stats\n        )\n\n    def forward(self, x):\n        if self.stride == 2:\n            x = self.relu(x)\n            y = self.pad(x)\n            out = torch.cat([self.convs[0](x), self.convs[1](y[:, :, 1:, 1:])], dim=1)\n        else:\n            out = self.conv(x)\n        out = self.bn(out)\n        return out\n\n    def extra_repr(self):\n        return \"C_in={C_in}, C_out={C_out}, stride={stride}\".format(**self.__dict__)\n\nclass MixedOp(nn.Module):\n    def __init__(self, ops, C_in, C_out, stride=1, affine= True, track_running_stats= True):\n        super(MixedOp, self).__init__()\n        self.C_in = C_in\n        self.C_out = C_out\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        self.stride = stride\n        self.ops = nn.ModuleList()\n        for op in ops:\n            cuda_op = OPS[op](C_in, C_out, stride, affine, track_running_stats)\n            self.ops.append(cuda_op)\n\n    def forward(self, x):\n        rx = torch.zeros_like(x)\n        for st_op in self.ops:\n            rx = rx + st_op(x)\n        return rx\n\nclass Cell(nn.Module):\n    def __init__(self, Arch, C_in, C_out, stride, affine= True, track_running_stats= True):\n        super(Cell, self).__init__()\n        self.Arch = Arch\n        self.C_in = C_in\n        self.C_out = C_out\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        self.stride = stride\n        self.out_dim = C_out\n        self.ops = nn.ModuleList()\n        for i in range(6):\n            ops = node_ops[self.Arch[i]]\n            st_op = MixedOp(ops, C_in, C_out)\n            self.ops.append(st_op)\n\n    def forward(self, x):\n        layers = [torch.zeros_like(x)]*6\n        layers[0] = x\n        for i in range(6):\n            s, t = L[i]\n            st_op = self.ops[i]\n            layers[t] = layers[t] + st_op(layers[s])\n        return layers[3]\n\nclass ResNetBasicblock(nn.Module):\n    def __init__(self, inplanes, planes, stride, affine=True, track_running_stats=True):\n        super(ResNetBasicblock, self).__init__()\n        assert stride == 1 or stride == 2, \"invalid stride {:}\".format(stride)\n        self.conv_a = ReLUConvBN(\n            inplanes, planes, 3, stride, 1, 1, affine, track_running_stats\n        )\n        self.conv_b = ReLUConvBN(\n            planes, planes, 3, 1, 1, 1, affine, track_running_stats\n        )\n        if stride == 2:\n            self.downsample = nn.Sequential(\n                nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n                nn.Conv2d(\n                    inplanes, planes, kernel_size=1, stride=1, padding=0, bias=False\n                ),\n            )\n        elif inplanes != planes:\n            self.downsample = ReLUConvBN(\n                inplanes, planes, 1, 1, 0, 1, affine, track_running_stats\n            )\n        else:\n            self.downsample = None\n        self.in_dim = inplanes\n        self.out_dim = planes\n        self.stride = stride\n        self.num_conv = 2\n\n    def extra_repr(self):\n        string = \"{name}(inC={in_dim}, outC={out_dim}, stride={stride})\".format(\n            name=self.__class__.__name__, **self.__dict__\n        )\n        return string\n\n    def forward(self, inputs):\n\n        basicblock = self.conv_a(inputs)\n        basicblock = self.conv_b(basicblock)\n\n        if self.downsample is not None:\n            residual = self.downsample(inputs)\n        else:\n            residual = inputs\n        return residual + basicblock\n\nclass TinyNetwork(nn.Module):\n    def __init__(self, C, N, Arch, num_classes):\n        '''\n        Initial Macro parameters (according to NAS-Bench-201):\n        C: Input channel of 1st stack: 16\n        N: Number of DAG-Cell/stack: 5\n        num_classes: =10 on CIFAR-10 (probably?)\n        '''\n        super(TinyNetwork, self).__init__()\n        self._C = C\n        self._layerN = N\n        self.Arch = Arch\n        self.num_classes = num_classes\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, C, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(C)\n        )\n\n        layer_channels = [C] * N + [C * 2] + [C * 2] * N + [C * 4] + [C * 4] * N\n        layer_reductions = [False] * N + [True] + [False] * N + [True] + [False] * N\n\n        C_prev = C\n        self.cells = nn.ModuleList()\n        for index, (C_curr, reduction) in enumerate(\n            zip(layer_channels, layer_reductions)\n        ):\n            if reduction:\n                cell = ResNetBasicblock(C_prev, C_curr, 2, True)\n            else:\n                cell = Cell(Arch, C_prev, C_curr, 1)\n            self.cells.append(cell)\n            C_prev = cell.out_dim\n        self._Layer = len(self.cells)\n\n        self.lastact = nn.Sequential(nn.BatchNorm2d(C_prev), nn.ReLU(inplace=True))\n        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(C_prev, num_classes)\n\n    def get_message(self):\n        string = self.extra_repr()\n        for i, cell in enumerate(self.cells):\n            string += \"\\n {:02d}/{:02d} :: {:}\".format(\n                i, len(self.cells), cell.extra_repr()\n            )\n        return string\n\n    def extra_repr(self):\n        return \"{name}(C={_C}, N={_layerN}, L={_Layer})\".format(\n            name=self.__class__.__name__, **self.__dict__\n        )\n\n    def forward(self, inputs):\n        feature = self.stem(inputs)\n        for i, cell in enumerate(self.cells):\n            feature = cell(feature)\n\n        out = self.lastact(feature)\n        out = self.global_pooling(out)\n        out = out.view(out.size(0), -1)\n        logits = self.classifier(out)\n\n        return out","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-30T13:37:16.287216Z","iopub.execute_input":"2022-12-30T13:37:16.287581Z","iopub.status.idle":"2022-12-30T13:37:16.335894Z","shell.execute_reply.started":"2022-12-30T13:37:16.287550Z","shell.execute_reply":"2022-12-30T13:37:16.334758Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn.functional as F\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nnum_epochs = 2\nbatch_size = 256\n\ntrainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n                                        download=True, transform=transform)\ntrain_indices = range(25000)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=False,sampler=train_indices)\n#tao sẽ thêm phần validation sau\n\nval_indices = range(25000,50000)\nvalloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=False,sampler=val_indices)\n\nfinal_indices = range(50000)\nfinalloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=False,sampler=final_indices)\n\ntestset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ndef get_metric(Arch, final= False):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    net = TinyNetwork(16,5,Arch,10)\n    net.to(device)\n    criterion  = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.1,momentum=0.9,nesterov=True,weight_decay=0.0005)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 200,eta_min=0)\n\n    global num_epochs\n    \n    if final:\n        num_epochs = 12\n    else:\n        num_epochs = 2\n    \n    for epoch in range(num_epochs):\n        net.train()\n        scheduler.step()\n        loader = trainloader\n        if final:\n            loader = finalloader\n        for inputs, labels in loader:\n            inputs,labels = inputs.to(device),labels.to(device)\n            # get the inputs; data is a list of [inputs, labels]\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs.float())\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        net.eval()\n        loader = valloader\n        if final:\n            loader = testloader\n        for inputs, labels in loader: #về sau ở đây t sẽ thay bằng validation\n            inputs,labels = inputs.to(device),labels.to(device)\n            outputs = net(inputs.float())\n            loss = criterion(outputs,labels)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.cpu().detach().size(0)\n            correct += (predicted == labels).sum().item()\n\n        acc = float(correct/total)\n        if final:\n            print('Final Judge:')\n        print(f'Accuracy of the network on the 10000 test images: {100 * acc} %')\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-12-30T13:37:30.796114Z","iopub.execute_input":"2022-12-30T13:37:30.796484Z","iopub.status.idle":"2022-12-30T13:37:32.298053Z","shell.execute_reply.started":"2022-12-30T13:37:30.796453Z","shell.execute_reply":"2022-12-30T13:37:32.297131Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport time\n# from nas_201_api import NASBench201API as API\n\n# Operation tree design\nnode_ops = [[]]*9 # operations on id-th node\ne = [[]]*9 # edge list\npar = [-1]*9 # parent list\n\ne[0] = [1,2]\ne[2] = [3,4]\ne[3] = [5,6]\ne[4] = [7,8]\n\npar[1] = 0\npar[2] = 0\npar[3] = 2\npar[4] = 2\npar[5] = 3\npar[6] = 3\npar[7] = 4\npar[8] = 4\n\n\nnode_ops[0] = ['none','nor_conv_1x1','nor_conv_3x3','skip_connect','avg_pool_3x3']\nnode_ops[1] = ['none']\nnode_ops[2] = ['nor_conv_1x1','nor_conv_3x3','skip_connect','avg_pool_3x3']\nnode_ops[3] = ['nor_conv_1x1','nor_conv_3x3']\nnode_ops[4] = ['skip_connect','avg_pool_3x3']\nnode_ops[5] = ['nor_conv_1x1']\nnode_ops[6] = ['nor_conv_3x3']\nnode_ops[7] = ['skip_connect']\nnode_ops[8] = ['avg_pool_3x3']\n\n# Cell's DAG edge list\nL = [(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)]\n\n# Encoding candidates and operations\nid_op = ['none','nor_conv_1x1','nor_conv_3x3','skip_connect','avg_pool_3x3']\nmasks = []\n\ndef To_mask(num):\n    res = ''\n    while(num > 0):\n        res += str(num % 2)\n        num = int(num/2)\n    while(len(res) < 6):\n        res += '0'\n    res = res[::-1]\n    return res\n\ndef Build_masks():\n    mask_len = 2**6 - 1\n    for i in range(mask_len + 1):\n        masks.append(To_mask(i))\n    print(\"Build_masks: Done\")\n\ndef Build_cand_arch(Arch, mask):\n    Cand_arch = []\n    Len = len(mask)\n    for i in range(Len):\n        if len(e[Arch[i]]) < 2:\n            Cand_arch.append(Arch[i])\n            continue\n        o0 = e[Arch[i]][0]\n        o1 = e[Arch[i]][1]\n        b = mask[i]\n        op = o0\n        if b == '1':\n            op = o1\n        Cand_arch.append(op)\n    return Cand_arch\n\ndef Score(Arch, final= False):\n    return get_metric(Arch, final)\n\ndef Check_connected(Arch):\n    '''\n    if (Arch[0] == 1) and (Arch[1] == 1) and (Arch[2] == 1):\n        return False\n    if (Arch[4] == 1) and (Arch[5] == 1):\n        return False\n    '''\n    fr = [True]*4\n    fr[0] = False\n    for i in range(6):\n        s, t = L[i]\n        if Arch[i] == 1:\n            continue\n        fr[t] = fr[s]\n    return not fr[3]\n\ndef BFS_T_o():\n    Arch = [0,0,0,0,0,0]\n    q = []\n    q.insert(0,0) # queue.push()\n    for i in range(3):\n        Cur_arch = Build_cand_arch(Arch, To_mask(0))\n        print(i, Cur_arch)\n        score = Score(Cur_arch)\n        for mask in masks:\n            Cand_arch = Build_cand_arch(Arch, mask)\n            print(mask, Cand_arch, Check_connected(Cand_arch))\n            if not Check_connected(Cand_arch):\n                continue\n            if Cur_arch == Cand_arch: \n                continue\n            cand_score = Score(Cand_arch)\n            if score < cand_score:\n                Cur_arch = Cand_arch\n                score = cand_score\n        Arch = Cur_arch\n    print(Arch)\n    final_score = Score(Arch, final= True)\n\nBuild_masks()\nBFS_T_o()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T13:37:39.638451Z","iopub.execute_input":"2022-12-30T13:37:39.638839Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Build_masks: Done\n0 [1, 1, 1, 1, 1, 1]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the 10000 test images: 10.036000000000001 %\n000000 [1, 1, 1, 1, 1, 1] False\n000001 [1, 1, 1, 1, 1, 2] False\n000010 [1, 1, 1, 1, 2, 1] False\n000011 [1, 1, 1, 1, 2, 2] False\n000100 [1, 1, 1, 2, 1, 1] False\n000101 [1, 1, 1, 2, 1, 2] False\n000110 [1, 1, 1, 2, 2, 1] False\n000111 [1, 1, 1, 2, 2, 2] False\n001000 [1, 1, 2, 1, 1, 1] True\nAccuracy of the network on the 10000 test images: 32.504 %\n001001 [1, 1, 2, 1, 1, 2] False\n001010 [1, 1, 2, 1, 2, 1] False\n001011 [1, 1, 2, 1, 2, 2] False\n001100 [1, 1, 2, 2, 1, 1] True\nAccuracy of the network on the 10000 test images: 38.635999999999996 %\n001101 [1, 1, 2, 2, 1, 2] False\n001110 [1, 1, 2, 2, 2, 1] False\n001111 [1, 1, 2, 2, 2, 2] False\n010000 [1, 2, 1, 1, 1, 1] False\n010001 [1, 2, 1, 1, 1, 2] True\nAccuracy of the network on the 10000 test images: 40.92 %\n010010 [1, 2, 1, 1, 2, 1] False\n010011 [1, 2, 1, 1, 2, 2] True\nAccuracy of the network on the 10000 test images: 34.252 %\n010100 [1, 2, 1, 2, 1, 1] False\n010101 [1, 2, 1, 2, 1, 2] False\n010110 [1, 2, 1, 2, 2, 1] False\n010111 [1, 2, 1, 2, 2, 2] False\n011000 [1, 2, 2, 1, 1, 1] True\n","output_type":"stream"}]}]}